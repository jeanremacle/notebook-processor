# CLAUDE.md Addendum â€” Package Ingestion Module

## Context

This addendum extends the notebook-processor with a **package ingestion module**.
Append these tasks after the existing Task 2.12 in the main CLAUDE.md.

The ingestion tool transforms a raw assignment folder into a clean, normalized
**notebook package** ready for the processing pipeline.

## Motivation (Real-World Findings)

Analysis of a real JHU assignment notebook revealed:

- **94.7%** of the notebook file was base64-encoded sample output images
- CSV data file encoded as Windows-1252 with smart quotes (0x92â†’', 0x96â†’â€“, 0x97â†’â€”)
- Notebook code uses `encoding='latin-1'` which silently corrupts characters
- TODO markers are `<-- YOUR SYSTEM PROMPT GOES HERE -->` inside Python string literals â€” not standard patterns
- External dependencies: `openai`, `pandas`, `config.json` for API keys
- Hardcoded file paths: `pd.read_csv("your_file_location", ...)`

## Package Structure

A notebook package is a self-contained folder under `assignments/`:

```text
assignments/
â””â”€â”€ w9-email-prioritization/          # One package = one folder
    â”œâ”€â”€ manifest.json                  # Generated by ingestion tool
    â”œâ”€â”€ notebook.ipynb                 # Cleaned notebook (images extracted)
    â”œâ”€â”€ notebook.ipynb.orig            # Original notebook (preserved)
    â”œâ”€â”€ instructions.md                # External instructions (if provided)
    â”œâ”€â”€ data/
    â”‚   â”œâ”€â”€ emails.csv                 # Cleaned (UTF-8)
    â”‚   â””â”€â”€ emails.csv.orig            # Original (Windows-1252, preserved)
    â”œâ”€â”€ images/
    â”‚   â”œâ”€â”€ sample_output_cell38.png   # Extracted from notebook
    â”‚   â”œâ”€â”€ sample_output_cell49.png
    â”‚   â””â”€â”€ ...
    â”œâ”€â”€ supplements/                   # Additional materials (rubric, etc.)
    â”‚   â””â”€â”€ rubric.pdf
    â”œâ”€â”€ transformations.log            # All changes made during ingestion
    â””â”€â”€ output/                        # Pipeline output (generated later)
```

## Pydantic Models

Add to `src/notebook_processor/models.py`:

```python
from decimal import Decimal


class AssetType(str, Enum):
    NOTEBOOK = "notebook"
    DATA = "data"
    IMAGE = "image"
    INSTRUCTIONS = "instructions"
    SUPPLEMENT = "supplement"


class DataQualityIssue(str, Enum):
    ENCODING = "encoding"
    LINE_ENDINGS = "line_endings"
    MISSING_VALUES = "missing_values"
    INCONSISTENT_TYPES = "inconsistent_types"


class DataSchema(BaseModel):
    """Schema summary for a data file â€” never the full content."""
    columns: list[str]
    dtypes: dict[str, str] = {}
    row_count: int
    sample_head: list[dict] = []   # First 3 rows as dicts
    sample_tail: list[dict] = []   # Last 2 rows as dicts
    null_counts: dict[str, int] = {}


class DataTransformation(BaseModel):
    """Record of a single data transformation applied during ingestion."""
    original_path: str
    issue: DataQualityIssue
    action: str
    details: str
    records_affected: int = 0
    confidence: float = 1.0         # 1.0 = deterministic, <1.0 = LLM-assisted
    backup_path: str | None = None


class ExtractedImage(BaseModel):
    """An image extracted from an inline base64 notebook cell."""
    cell_index: int
    original_size_bytes: int
    extracted_path: str
    description: str | None = None  # Will be filled by vision model later
    purpose: str = "sample_output"  # sample_output | diagram | instruction


class Asset(BaseModel):
    """A single file within the notebook package."""
    path: str
    type: AssetType
    format: str
    size_bytes: int
    schema: DataSchema | None = None
    contents: list[str] | None = None       # For ZIP archives
    description: str | None = None
    transformations: list[DataTransformation] = []
    extracted_images: list[ExtractedImage] = []


class TodoMarker(BaseModel):
    """A detected TODO cell in the notebook."""
    cell_index: int
    cell_type: CellType
    marker_pattern: str       # The pattern that triggered detection
    variable_name: str | None = None  # e.g., "system_prompt", "user_prompt"
    task_id: str | None = None        # e.g., "1A", "1B", "2", "3"
    context: str = ""                 # Surrounding cell content for reference


class NotebookAnalysis(BaseModel):
    """Analysis results from notebook preprocessing."""
    total_cells: int
    cell_type_counts: dict[str, int]
    todo_markers: list[TodoMarker]
    embedded_images_count: int
    embedded_images_total_bytes: int
    dependencies: list[str] = []        # Detected pip packages
    api_dependencies: list[str] = []    # Detected API services (openai, etc.)
    hardcoded_paths: list[str] = []     # File paths that need patching
    kernel_spec: str | None = None


class InstructionImprovement(BaseModel):
    """Record of an instruction that was clarified or split."""
    original_text: str
    improved_text: str
    sub_steps: list[str] = []
    rationale: str


class PackageMetadata(BaseModel):
    """Top-level package metadata."""
    id: str                           # Folder name / slug
    name: str                         # Human-readable name
    course: str = ""
    created_at: str                   # ISO 8601
    status: str = "pending"           # pending | processing | done | failed


class NotebookMetadata(BaseModel):
    """Notebook-specific metadata."""
    filename: str
    original_filename: str
    kernel: str = "python3"
    requires_gpu: bool = False
    analysis: NotebookAnalysis | None = None


class ModelConfig(BaseModel):
    """Model configuration for delivery vs optimization."""
    delivery: str = ""                # Model required by assignment
    default: str = ""                 # Default for optimization


class BenchmarkConfig(BaseModel):
    """Benchmark configuration."""
    enabled: bool = False
    iterations: list[str] = []


class PackageManifest(BaseModel):
    """Complete manifest for a notebook package."""
    package: PackageMetadata
    notebook: NotebookMetadata
    assets: list[Asset]
    instruction_improvements: list[InstructionImprovement] = []
    model_config: ModelConfig = ModelConfig()
    benchmark: BenchmarkConfig = BenchmarkConfig()
    outputs: list[str] = []
```

## New Module Structure

```text
src/notebook_processor/
â”œâ”€â”€ ingestion/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ ingestor.py         # Main orchestrator: PackageIngestor.ingest(raw_dir) -> Path
â”‚   â”œâ”€â”€ inventory.py        # Phase 1: scan folder, identify files
â”‚   â”œâ”€â”€ data_repair.py      # Phase 2: encoding detection, conversion, validation
â”‚   â”œâ”€â”€ notebook_preprocess.py  # Phase 3: image extraction, path patching, marker detection
â”‚   â”œâ”€â”€ instruction_improver.py # Phase 4: clarify and split instructions (LLM-assisted)
â”‚   â”œâ”€â”€ manifest_generator.py   # Phase 5: assemble manifest.json
â”‚   â””â”€â”€ transformations.py      # Transformation logger
```

## Task Sequence

### Task 2.13 â€” Package Models + Transformation Logger

- Add all Pydantic models above to `models.py`
- Implement `TransformationLogger` in `ingestion/transformations.py`:
  - Append-only log of `DataTransformation` records
  - `log(transformation: DataTransformation) -> None`
  - `save(path: Path) -> None` â€” writes `transformations.log` as human-readable text
  - `get_summary() -> str` â€” one-line summary for manifest
- Write tests
- Commit: `feat: add package ingestion models and transformation logger`

### Task 2.14 â€” Inventory Scanner

- Implement `InventoryScanner` in `ingestion/inventory.py`:
  - `scan(raw_dir: Path) -> list[Asset]`
  - Identifies files by extension and content:
    - `.ipynb` â†’ `AssetType.NOTEBOOK`
    - `.csv`, `.tsv`, `.parquet`, `.json` (data) â†’ `AssetType.DATA`
    - `.png`, `.jpg`, `.gif`, `.svg` â†’ `AssetType.IMAGE`
    - `.md`, `.txt` (non-data) â†’ `AssetType.INSTRUCTIONS`
    - `.pdf`, `.docx`, everything else â†’ `AssetType.SUPPLEMENT`
  - For CSV files: extract `DataSchema` (read with pandas, sample head/tail, null counts)
  - For ZIP files: list contents without extracting
  - Records file sizes
- Write tests with fixtures (small CSV, notebook, image, ZIP)
- Commit: `feat: add inventory scanner for package ingestion`

### Task 2.15 â€” Data Repair

- Implement `DataRepairer` in `ingestion/data_repair.py`:
  - `repair(asset: Asset, package_dir: Path, logger: TransformationLogger) -> Asset`
  - Encoding detection via `charset-normalizer` (prefer over chardet â€” more accurate):
    - If not UTF-8: convert, log the transformation, preserve `.orig`
    - Character mapping for Windows-1252 â†’ UTF-8:
      - `0x92` â†’ `'` (RIGHT SINGLE QUOTATION MARK, U+2019)
      - `0x93` â†’ `"` (LEFT DOUBLE QUOTATION MARK, U+201C)
      - `0x94` â†’ `"` (RIGHT DOUBLE QUOTATION MARK, U+201D)
      - `0x96` â†’ `â€“` (EN DASH, U+2013)
      - `0x97` â†’ `â€”` (EM DASH, U+2014)
    - Log each unique character substitution with count
  - Line ending normalization: `\r\n` â†’ `\n`
  - CSV validation: attempt `pd.read_csv()` with detected encoding, report errors
  - Update the `Asset` with transformations applied
- Add `charset-normalizer>=3.0` to dependencies
- Write tests with the actual encoding patterns found (Windows-1252 smart quotes)
- Commit: `feat: add data repair with encoding detection and conversion`

### Task 2.16 â€” Notebook Preprocessing

- Implement `NotebookPreprocessor` in `ingestion/notebook_preprocess.py`:
  - `preprocess(notebook_path: Path, package_dir: Path, logger: TransformationLogger) -> NotebookAnalysis`

  **Phase 3a â€” Extract embedded images:**
  - Scan all cells for `data:image/...;base64,...` patterns
  - Extract each base64 payload â†’ decode â†’ save to `images/sample_output_cell{N}.png`
  - Replace inline base64 in cell source with: `![Sample Output](images/sample_output_cell{N}.png)`
  - Log: `[IMAGE] Cell {N}: extracted {size} KB base64 image â†’ images/sample_output_cell{N}.png`

  **Phase 3b â€” Detect TODO markers (configurable):**
  - Default marker patterns (regex):

    ```python
    DEFAULT_TODO_PATTERNS = [
        # Standard patterns (from demo notebook)
        r"#\s*TODO",
        r"raise\s+NotImplementedError",
        r"#\s*YOUR\s+CODE\s+HERE",
        r"\*\*Your\s+answer\s+here\*\*",
        r"\*\*YOUR\s+ANSWER\s+HERE\*\*",
        # JHU patterns (from real notebook)
        r"<--\s*YOUR\s+SYSTEM\s+PROMPT\s+GOES\s+HERE\s*-->",
        r"<--\s*YOUR\s+USER\s+PROMPT\s+GOES\s+HERE\s*-->",
        r"`<Enter\s+your\s+.*here>`",
        # Generic
        r"pass\s*$",  # Bare pass at end of function
    ]
    ```

  - For each match, extract variable name from code context (e.g., `system_prompt`, `user_prompt`)
  - Attempt to map to task ID from surrounding markdown headings
  - Produce `list[TodoMarker]`

  **Phase 3c â€” Detect dependencies:**

  - Scan code cells for `import X`, `from X import`, `!pip install X`
  - Identify API dependencies: `openai`, `anthropic`, `google.generativeai`, etc.
  - Detect `config.json` or similar credential file references

  **Phase 3d â€” Detect hardcoded paths:**

  - Scan for string literals containing file paths: `"your_file_location"`, `"data.csv"`, `"/path/to/"`
  - Record for later patching by the solver

  **Phase 3e â€” Save cleaned notebook:**

  - Write the cleaned notebook (images extracted) as `notebook.ipynb`
  - Preserve original as `notebook.ipynb.orig`

- Write tests for each sub-phase
- Commit: `feat: add notebook preprocessor with image extraction and marker detection`

### Task 2.17 â€” Instruction Improver (Stub)

- Implement `InstructionImprover` in `ingestion/instruction_improver.py`:
  - `improve(notebook_analysis: NotebookAnalysis, instructions_path: Path | None, rubric: str | None) -> list[InstructionImprovement]`
  - **MVP: stub implementation** that returns empty list
  - **Interface ready for LLM-assisted improvement** in Phase 4:
    - Merges notebook markdown instructions with external `.md` file
    - Identifies vague instructions
    - Proposes sub-step decomposition
    - Maps rubric criteria to measurable objectives
  - The LLM call will go through the Solver interface (same abstraction)
- Write tests for the interface
- Commit: `feat: add instruction improver interface (stub for MVP)`

### Task 2.18 â€” Manifest Generator

- Implement `ManifestGenerator` in `ingestion/manifest_generator.py`:
  - `generate(package_dir: Path, assets: list[Asset], analysis: NotebookAnalysis, improvements: list[InstructionImprovement]) -> PackageManifest`
  - Assembles all ingestion outputs into `manifest.json`
  - Writes `manifest.json` to package root
  - Validates manifest against Pydantic model
- Write tests
- Commit: `feat: add manifest generator`

### Task 2.19 â€” Package Ingestor (Orchestrator)

- Implement `PackageIngestor` in `ingestion/ingestor.py`:
  - `ingest(raw_dir: Path, target_dir: Path) -> PackageManifest`
  - Chains: inventory â†’ data_repair â†’ notebook_preprocess â†’ instruction_improve â†’ manifest_generate
  - Creates the target package directory structure
  - Copies and organizes files into canonical layout
  - Runs all phases, logs everything
  - Returns the complete manifest
- Write integration test using fixtures that simulate the JHU notebook scenario:
  - A small `.ipynb` with one base64 image and one TODO marker
  - A small `.csv` with Windows-1252 encoding
  - An `instructions.md` file
- Commit: `feat: add PackageIngestor orchestrator`

### Task 2.20 â€” CLI Extension + Demo

- Extend CLI (`cli.py`) with `ingest` command:
  - `notebook-processor ingest --input raw/ --output assignments/pkg-name/`
  - Runs the full ingestion pipeline
  - Prints summary: files found, transformations applied, markers detected
- Create `examples/jhu-demo/raw/` with minimal fixtures mimicking the real notebook:
  - Small `.ipynb` with base64 image + `<-- YOUR SYSTEM PROMPT GOES HERE -->`
  - Small `.csv` with Windows-1252 smart quotes
  - `instructions.md`
- Run `make ingest INPUT=examples/jhu-demo/raw/ OUTPUT=examples/jhu-demo/assignments/demo/`
- Verify: manifest.json correct, images extracted, CSV clean, markers detected
- Update Makefile with `ingest` target
- Commit: `feat: add ingest CLI command and JHU-style demo`

## Key Dependencies to Add

```toml
# In pyproject.toml [project] dependencies
"charset-normalizer>=3.0",   # Better encoding detection than chardet
```

## Design Decisions for This Module

1. **Why charset-normalizer over chardet?** â€” More accurate for Western European encodings, actively maintained, faster. It correctly identifies Windows-1252 vs Latin-1 vs UTF-8.
2. **Why extract images to files?** â€” Reduces notebook from 717 KB to ~38 KB. The solver gets image descriptions (via vision model) rather than raw pixels. The images remain available for the final report.
3. **Why configurable TODO patterns?** â€” Every course has different conventions. Hardcoding breaks on the first real notebook. Regex patterns in a list are easy to extend.
4. **Why preserve .orig files?** â€” Academic integrity. The professor can verify what was changed. The report references transformations.
5. **Why stub the instruction improver?** â€” It needs LLM access (Solver interface). Phase 4 will implement it when the model router is available. The interface is ready now.

## Session Communication Protocol

### PM Communication Protocol

#### Inbound â€” directives from PM layer (read-only)

At **session start** and before beginning any new step, read `directives.md` in the project root.
This file contains current priorities, decisions, and constraints set by the project management layer.

**Rules:**

- Never modify `directives.md` â€” it is owned by the PM layer
- If directives conflict with `PLAN.md`, follow directives (they represent latest decisions)
- If directives are absent or empty, follow `PLAN.md` as-is

#### Outbound â€” journal to PM layer (append-only)

After completing each implementation step, or when encountering a blocker,
**append** a timestamped entry to `journal.md` in the project root:

```markdown
### {ISO-8601 timestamp} â€” Step {N}: {short title}
**Status**: âœ… Complete | ğŸš§ In Progress | âŒ Blocked
**Changes**: {list of files created or modified}
**Next**: {intended next step, referencing PLAN.md}
**Blockers**: {None | description of what needs a PM decision}
```

**Rules:**

- **Never rewrite or delete** existing entries â€” append only
- Create the file if it does not exist
- Keep entries factual and concise
- Reference `PLAN.md` step numbers for traceability

#### Deliverables Boundary

All code artifacts (source, tests, configs, plans) stay in this repo.
Never write files outside the repo root.
Never write files into `../project-management/`.
